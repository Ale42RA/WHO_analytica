{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5592c1d5",
   "metadata": {},
   "source": [
    "# Analytica - Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef39f7b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200677bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import core libraries\n",
    "\n",
    "# For data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# For train/test splitting and scaling processes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# For modelling (specifically for generating the constant column as part of FE)\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64734a0",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80179d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data set\n",
    "\n",
    "# Create data frame from the WHO data\n",
    "who = pd.read_csv(\"life_expectancy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preview the data frame\n",
    "\n",
    "who.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperate features and target for train/test splitting\n",
    "\n",
    "# Split the target from the columns\n",
    "feature_cols = list(who.columns)\n",
    "feature_cols.remove('Life_expectancy')\n",
    "\n",
    "# Create X (features), and y (target) variables.\n",
    "X = who[feature_cols]\n",
    "y = who['Life_expectancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the train-test split function from sklearn\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify= who['Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b58e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visually check that the number of records between X and y (across Train and Test) both match\n",
    "\n",
    "print(f\"X_train:   {X_train.shape}\\ny_train:   {y_train.shape}\\n\")\n",
    "print(f\"X_test:    {X_test.shape}\\ny_test:    {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assert that the indicies and number of records between X and y (across Train and Test) both match\n",
    "\n",
    "# Indicies\n",
    "assert(all(X_train.index == y_train.index)), \"There is some index mismatch in Train\"\n",
    "assert(all(X_test.index == y_test.index)), \"There is some index mismatch in Test\"\n",
    "\n",
    "# Number of records\n",
    "assert(X_train.shape[0] == y_train.shape[0]), \"There is some records mismatch in Train\"\n",
    "assert(X_test.shape[0] == y_test.shape[0]), \"There is some records mismatch in Test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be2279",
   "metadata": {},
   "source": [
    "## Feature Engingeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code from the FE notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ae435",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engingeering process\n",
    "\n",
    "# Custom function for all FE processes\n",
    "def feature_eng(who):\n",
    "        # Create copy of the data frame\n",
    "        who = who.copy()\n",
    "        \n",
    "        # One Hot Encoding (OHE)\n",
    "        who = pd.get_dummies(who, columns = ['Region'], drop_first = True, prefix = 'Region', dtype = int)\n",
    "\n",
    "        # Converting features into logarithm\n",
    "        who['Incidents_HIV_log'] = np.log(who['Incidents_HIV'])\n",
    "        who['GDP_per_capita_log'] = np.log(who['GDP_per_capita'])\n",
    "\n",
    "        # Create standard scaler variables           \n",
    "        standard_scaler_bmi = StandardScaler()\n",
    "        standard_scaler_schooling = StandardScaler()\n",
    "        \n",
    "        # Create minmax scaler variables   \n",
    "        minmax_scaler_gdp = MinMaxScaler()\n",
    "        minmax_scaler_hiv = MinMaxScaler()\n",
    "        \n",
    "        # Create robust scaler variables \n",
    "        robust_scaler_under_five = RobustScaler()\n",
    "        robust_scaler_adult_mortality = RobustScaler()\n",
    "    \n",
    "        # Normally distributed feature: BMI\n",
    "        who[['BMI']] = standard_scaler_bmi.fit_transform(who[['BMI']])\n",
    "    \n",
    "        # Normally distributed feature: Schooling\n",
    "        who[['Schooling']] = standard_scaler_schooling.fit_transform(who[['Schooling']])\n",
    "    \n",
    "        # MinMax scaling for bounded features\n",
    "        who[['GDP_per_capita_log']] = minmax_scaler_gdp.fit_transform(who[['GDP_per_capita_log']])\n",
    "        who[['Incidents_HIV_log']] = minmax_scaler_hiv.fit_transform(who[['Incidents_HIV_log']])\n",
    "    \n",
    "        # Robust scaling for features with outliers\n",
    "        who[['Under_five_deaths']] = robust_scaler_under_five.fit_transform(who[['Under_five_deaths']])\n",
    "        who[['Adult_mortality']] = robust_scaler_adult_mortality.fit_transform(who[['Adult_mortality']])\n",
    "    \n",
    "        # Save the scalers individually\n",
    "        joblib.dump(standard_scaler_bmi, 'scr/standard_scaler_bmi.pkl')\n",
    "        joblib.dump(standard_scaler_schooling, 'scr/standard_scaler_schooling.pkl')\n",
    "        joblib.dump(minmax_scaler_gdp, 'scr/minmax_scaler_gdp.pkl')\n",
    "        joblib.dump(minmax_scaler_hiv, 'scr/minmax_scaler_hiv.pkl')\n",
    "        joblib.dump(robust_scaler_under_five, 'scr/robust_scaler_under_five.pkl')\n",
    "        joblib.dump(robust_scaler_adult_mortality, 'scr/robust_scaler_adult_mortality.pkl')\n",
    "        \n",
    "        # Created for statsmodeling. Must always be present\n",
    "        who = sm.add_constant(who)\n",
    "\n",
    "        # Return the results\n",
    "        return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e86ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the X train and test data with the feature engineering\n",
    "\n",
    "X_train_fe = feature_eng(X_train)\n",
    "X_test_fe = feature_eng(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad76026",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visually check the transformation\n",
    "\n",
    "X_train_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that the X_train_fe contains no nulls and that all data types are ready for modelling\n",
    "\n",
    "# Null values\n",
    "print(f\"Sum of nulls: {sum(X_train_fe.isnull().sum())}\")\n",
    "\n",
    "# Data \n",
    "X_train_fe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precise model feature columns used\n",
    "\n",
    "feature_cols_pre = [\n",
    " 'const',\n",
    " 'Under_five_deaths',\n",
    " 'Adult_mortality',\n",
    " 'BMI',\n",
    " 'Schooling',\n",
    " 'Region_Asia',\n",
    " 'Region_Central America and Caribbean',\n",
    " 'Region_European Union',\n",
    " 'Region_Middle East',\n",
    " 'Region_North America',\n",
    " 'Region_Oceania',\n",
    " 'Region_Rest of Europe',\n",
    " 'Region_South America',\n",
    " 'Incidents_HIV_log',\n",
    " 'GDP_per_capita_log'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimalistic model feature columns used\n",
    "\n",
    "feature_cols_min = [\n",
    " 'const',\n",
    " 'Under_five_deaths',\n",
    " 'Adult_mortality',\n",
    " 'BMI'\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd37ea",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7357550c",
   "metadata": {},
   "source": [
    "### Precise Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd029fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit train set on the precise model and check summary\n",
    "\n",
    "lin_reg = sm.OLS(y_train, X_train_fe[feature_cols_pre])\n",
    "results = lin_reg.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa004699",
   "metadata": {},
   "source": [
    "### Precise Model Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict results of the minimalistic model\n",
    "\n",
    "# Train set\n",
    " \n",
    "y_pred = results.predict(X_train_fe[feature_cols_pre])\n",
    "y_pred_rmse = statsmodels.tools.eval_measures.rmse(y_train, y_pred)\n",
    "\n",
    "# Test test\n",
    "y_test_pred = results.predict(X_test_fe[feature_cols_pre])\n",
    "y_test_pred_rmse = statsmodels.tools.eval_measures.rmse(y_test, y_test_pred)\n",
    "\n",
    "# Print RMSE values\n",
    "print(f'Train RMSE  = {y_pred_rmse}')\n",
    "print(f'Test RMSE   = {(y_test_pred_rmse)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8d21c",
   "metadata": {},
   "source": [
    "### Minimalistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit train set on the minimalistic model and check summary\n",
    "\n",
    "lin_reg = sm.OLS(y_train, X_train_fe[feature_cols_min])\n",
    "results = lin_reg.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict results of the minimalistic model\n",
    "\n",
    "# Train set\n",
    " \n",
    "y_pred = results.predict(X_train_fe[feature_cols_min])\n",
    "y_pred_rmse = statsmodels.tools.eval_measures.rmse(y_train, y_pred)\n",
    "\n",
    "# Test test\n",
    "y_test_pred = results.predict(X_test_fe[feature_cols_min])\n",
    "y_test_pred_rmse = statsmodels.tools.eval_measures.rmse(y_test, y_test_pred)\n",
    "\n",
    "# Print RMSE values\n",
    "print(f'Train RMSE  = {y_pred_rmse}')\n",
    "print(f'Test RMSE   = {(y_test_pred_rmse)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
